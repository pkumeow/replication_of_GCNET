{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# 删除100个股票中不全的留下81个\n",
    "# 按照论文的说法2011-2020 10年数据都不缺\n",
    "path = '100 stocks/'\n",
    "stocks = os.listdir(path)\n",
    "for i in stocks:\n",
    "    if len(pd.read_csv(path+i))!=2518:\n",
    "        os.remove(path+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "# 输入：股票原始数据，output='PLD'/'node_representation'；输出：返回特征处理后的对应列\n",
    "def process_feature(df, output):\n",
    "    # 计算CCI指标\n",
    "    df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "\n",
    "    # 计算SAR指标\n",
    "    df['SAR'] = talib.SAR(df['High'], df['Low'], acceleration=0.02, maximum=0.2)\n",
    "\n",
    "    # 计算ADX指标\n",
    "    df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "\n",
    "    # 计算MFI指标\n",
    "    df['MFI'] = talib.MFI(df['High'], df['Low'], df['Close'], df['Volume'], timeperiod=14)\n",
    "\n",
    "    # 计算RSI指标\n",
    "    df['RSI'] = talib.RSI(df['Close'], timeperiod=14)\n",
    "\n",
    "    # 计算SK、SD指标\n",
    "    df['SK'], df['SD'] = talib.STOCH(df['High'], df['Low'], df['Close'], fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "\n",
    "    # 计算RSI指标信号\n",
    "    df['RSI_S'] = 0\n",
    "    df.loc[df['RSI'] > 70, 'RSI_S'] = -1 # RSI > 70时卖出信号\n",
    "    df.loc[df['RSI'] < 30, 'RSI_S'] = 1 # RSI < 30时买入信号\n",
    "\n",
    "    # 计算Bollinger bands指标信号\n",
    "    df['SD_upper'], middle, df['SD_lower'] = talib.BBANDS(df['Close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    df['BB_S'] = 0\n",
    "    df.loc[df['Close'] > df['SD_upper'], 'BB_S'] = -1 # 收盘价突破上轨带时卖出信号\n",
    "    df.loc[df['Close'] < df['SD_lower'], 'BB_S'] = 1 # 收盘价跌破下轨带时买入信号\n",
    "\n",
    "    # 计算MACD指标信号\n",
    "    macd, signal, df['MACD_hist'] = talib.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    df['MACD_S'] = 0\n",
    "    df.loc[(df['MACD_hist'] > 0) & (df['MACD_hist'].shift(1) < 0), 'MACD_S'] = 1 # MACD柱从负值向上穿过0轴时买入信号\n",
    "    df.loc[(df['MACD_hist'] < 0) & (df['MACD_hist'].shift(1) > 0), 'MACD_S'] = -1 # MACD柱从正值向下穿过0轴时卖出信号\n",
    "\n",
    "    # 计算SAR指标信号\n",
    "    df['SAR_S'] = 0\n",
    "    df.loc[df['Close'] > df['SAR'], 'SAR_S'] = 1 # 收盘价突破SAR时买入信号\n",
    "    df.loc[df['Close'] < df['SAR'], 'SAR_S'] = -1 # 收盘价跌破SAR时卖出信号\n",
    "\n",
    "    # 计算ADX指标信号\n",
    "    df['ADX_S'] = 0\n",
    "    df.loc[df['ADX'] > 25, 'ADX_S'] = 1 # ADX > 25时趋势明显，可以买卖信号\n",
    "    df.loc[df['ADX'] < 20, 'ADX_S'] = -1 # ADX < 20时趋势不明显，不建议买卖信号\n",
    "\n",
    "    # 计算Stochastic指标信号\n",
    "    df['S_S'] = 0\n",
    "    df.loc[(df['SK'] > df['SD']) & (df['SK'].shift() < df['SD'].shift()), 'S_S'] = 1 # Slow stoch %K上穿Slow stoch %D时买入信号\n",
    "    df.loc[(df['SK'] < df['SD']) & (df['SK'].shift() > df['SD'].shift()), 'S_S'] = -1 # Slow stoch %K下穿Slow stoch %D时卖出信号\n",
    "\n",
    "    # 计算MFI指标信号\n",
    "    df['MFI_S'] = 0\n",
    "    df.loc[df['MFI'] > 80, 'MFI_S'] = -1 # MFI > 80时卖出信号\n",
    "    df.loc[df['MFI'] < 20, 'MFI_S'] = 1 # MFI < 20时买入信号\n",
    "\n",
    "    # 计算CCI指标信号\n",
    "    df['CCI_S'] = 0\n",
    "    df.loc[df['CCI'] > 100, 'CCI_S'] = -1 # CCI > 100时卖出信号\n",
    "    df.loc[df['CCI'] < -100, 'CCI_S'] = 1 # CCI < -100时买入信号\n",
    "\n",
    "    # 计算交易量符号函数：Sign(Volume -Avg(last 5 days))\n",
    "    df['Average_Volume'] = df['Volume'].rolling(window=5).mean() # 计算5天的平均交易量并添加到DataFrame中\n",
    "    df['Volume_Diff'] = df['Volume'] - df['Average_Volume'] # 计算当日交易量相对于过去5天平均交易量的差值\n",
    "    df['V_S'] = df['Volume_Diff'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0)) # 计算交易量符号函数\n",
    "\n",
    "    # 计算开盘价与收盘价之差的符号函数：Sign(CP-OP)\n",
    "    df['CPOP'] = df['Close'] - df['Open'] # 计算当日收盘价与开盘价之差\n",
    "    df['CPOP_S'] = df['CPOP'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0)) # 计算开盘价与收盘价之差的符号函数\n",
    "\n",
    "    # 计算当日收盘价相对于昨日收盘价的符号函数：Sign(CP-Closing price yesterday)\n",
    "    df['CPCPY'] = df['Close'].diff() # 计算当日收盘价与昨日收盘价之差\n",
    "    df['CPCPY_S'] = df['CPCPY'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0)) # 计算当日收盘价相对于昨日收盘价的符号函数\n",
    "    \n",
    "    # 输出去掉含有缺失值的行\n",
    "    if output=='PLD':\n",
    "        return df[['Open', 'High', 'Low', 'Close', 'Volume', 'CCI', 'SAR', 'ADX', 'MFI', 'RSI', 'SK', 'SD', 'RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S']].dropna().reset_index(drop=True)\n",
    "    elif output=='node_representation':\n",
    "        df['label'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "        return df[['RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S','label']].reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 输入：两个公司的数据；输出：融合两部分信息的数据，字段格式一致\n",
    "def mix_ij(df_i, df_j):\n",
    "    cols = df_i.columns.tolist()\n",
    "    df_ij = pd.DataFrame(columns=cols)\n",
    "    for col in cols:\n",
    "        df_ij[col] = 1/2*(df_i[col]+df_j[col])\n",
    "    return df_ij\n",
    "\n",
    "# 输入：含有target列作为分类结果的数据；输出：QDA准确率\n",
    "def QDA_(df):\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    # 划分训练集和测试集\n",
    "    split = int(len(df)*0.8)\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # 训练QDA分类器\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(X_train, y_train)\n",
    "    # 在测试集上评估模型性能\n",
    "    accuracy = qda.score(X_test, y_test)\n",
    "    return accuracy\n",
    "\n",
    "# 输入：公司i数据和ij融合数据；输出：融合数据QDA准确率与单独准确率之增长\n",
    "def diff_acc(df_i, df_ij):\n",
    "    # 标签生成：如果下一时刻的收盘价高于当前时间点的收盘价，标记为1；否则标记为0\n",
    "    df_i['target'] = (df_i['Close'].shift(-1) > df_i['Close']).astype(int)\n",
    "    df_ij['target'] = df_i['target']\n",
    "    df_i = df_i[:-1]\n",
    "    df_ij = df_ij[:-1]\n",
    "    acc_i = QDA_(df_i)\n",
    "    acc_ij = QDA_(df_ij)\n",
    "    return (acc_ij - acc_i)\n",
    "\n",
    "# 输入：公司i数据和公司j数据；输出：边权重\n",
    "def influence(df_i, df_j):\n",
    "    df_i = process_feature(df_i, 'PLD')\n",
    "    df_j = process_feature(df_j, 'PLD')\n",
    "    df_ij = mix_ij(df_i, df_j)\n",
    "    return 1/2*(diff_acc(df_i, df_ij)+diff_acc(df_j, df_ij))\n",
    "\n",
    "import networkx as nx\n",
    "import itertools\n",
    "# 输入：观测日期，利用该日期之前的数据构建公司关系图；输出：联通图\n",
    "import functools\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def create_graph(observe_date):\n",
    "    # 创建一张无向图\n",
    "    G = nx.Graph()\n",
    "    for i, j in itertools.combinations(stocks, 2):\n",
    "        df_i = pd.read_csv(path+i)\n",
    "        df_j = pd.read_csv(path+j)\n",
    "        df_i = df_i[df_i['Date'] <= observe_date].sort_values('Date')\n",
    "        df_j = df_j[df_j['Date'] <= observe_date].sort_values('Date')\n",
    "        w = influence(df_i, df_j)\n",
    "        if w > 0:\n",
    "            # 添加正边和权重（相当于移除掉负权边）\n",
    "            G.add_edge(i.split('.')[0], j.split('.')[0], weight=w)\n",
    "    # 不断移除最小权重的边，直到图不再连通，将最后一条移除的边还原回去\n",
    "    while nx.is_connected(G):\n",
    "        # 获取权重最小的边，如果有多条边权重相同，则返回第一条边\n",
    "        min_edge = min(G.edges(data=True), key=lambda x: x[2]['weight'])\n",
    "        # 移除该边\n",
    "        G.remove_edge(min_edge[0], min_edge[1])\n",
    "    G.add_edge(min_edge[0], min_edge[1], weight=min_edge[2]['weight'])\n",
    "    # 确认图是连通图\n",
    "    if nx.is_connected(G)==True:\n",
    "        return G\n",
    "    else:\n",
    "        print('NOT connected!!!')\n",
    "# 归一化边权重，没有返回值，直接在输入G上操作\n",
    "def normalize_graph(G):\n",
    "    max_w = max([d['weight'] for (u, v, d) in G.edges(data=True)])\n",
    "    for (u, v, d) in G.edges(data=True):\n",
    "        d['weight'] /= max_w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_graph('2018-07-27')\n",
    "normalize_graph(G)\n",
    "import matplotlib.pyplot as plt\n",
    "# 找出科技和互联网行业的，演示作图的4个阶段\n",
    "node_list = ['AMZN', 'TSLA', 'INTC', 'GOOG', 'NVDA', 'NTES']\n",
    "subgraph = G.subgraph(node_list)\n",
    "plt.figure(figsize=(50,50))\n",
    "weights = nx.get_edge_attributes(subgraph, 'weight')\n",
    "for i in weights:\n",
    "    weights[i] = round(weights[i],2)\n",
    "pos = nx.spring_layout(subgraph)\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_size=20000, node_color='lightblue')\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size=40, font_family='sans-serif')\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph, \n",
    "    pos, \n",
    "    width=[w * 50 for w in weights.values()], \n",
    "    edgelist=weights.keys(),\n",
    "    edge_color='black')\n",
    "nx.draw_networkx_edge_labels(subgraph, pos, edge_labels=weights, font_size=50,label_pos=0.5, font_family='sans-serif')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入：无向图；输出：节点对应密度字典\n",
    "def density_score(G):\n",
    "    # 初始化结果字典\n",
    "    densities = {}\n",
    "    # 遍历每个节点\n",
    "    for node in G.nodes():\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        # 节点的度（即相邻节点数）\n",
    "        deg_i = len(neighbors)\n",
    "        # 如果节点度小于2，则密度为0\n",
    "        if deg_i <= 1:\n",
    "            densities[node] = 0\n",
    "        else:\n",
    "            # 计算节点的权重平均邻居度\n",
    "            sum_wdeg_jk = 0\n",
    "            for j, k in itertools.combinations(neighbors, 2):\n",
    "                w_ji = G[j][node]['weight']\n",
    "                w_ki = G[k][node]['weight']\n",
    "                w_jk = G[j][k]['weight'] if G.has_edge(j, k) else 0\n",
    "                sum_wdeg_jk += ((w_ji * w_ki * w_jk) ** (1/3))\n",
    "            wdeg_i = sum_wdeg_jk / (deg_i * (deg_i - 1))\n",
    "            # 计算节点的密度分数\n",
    "            densities[node] = wdeg_i\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLD用到的不同算法\n",
    "def LDA(X_train, X_test, y_train):\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    # 构建LDA模型\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    return lda, y_pred\n",
    "\n",
    "def DT(X_train, X_test, y_train):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    # 构建决策树模型\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train, y_train)\n",
    "    y_pred = dtc.predict(X_test)\n",
    "    return dtc, y_pred\n",
    "\n",
    "def GNB(X_train, X_test, y_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    # 构建高斯朴素贝叶斯模型\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    return gnb, y_pred\n",
    "\n",
    "def QDA(X_train, X_test, y_train):\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    # 构建QDA模型\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(X_train, y_train)\n",
    "    y_pred = qda.predict(X_test)\n",
    "    return qda, y_pred\n",
    "\n",
    "def RFC(X_train, X_test, y_train):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    # 构建随机森林模型\n",
    "    rfc = RandomForestClassifier(n_estimators=100)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    return rfc, y_pred\n",
    "\n",
    "def MLP(X_train, X_test, y_train):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    # 构建多层感知器模型\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=50000)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    return mlp, y_pred\n",
    "\n",
    "def adjust_score(y_pred, y_test, c):\n",
    "    length = len(y_test)\n",
    "    score = 0\n",
    "    for i in range(length):\n",
    "        if y_pred[i] == y_test[i]:\n",
    "            score+=(1-c)**(length-1-i)\n",
    "    return score\n",
    "\n",
    "# 输入：一股票数据；输出：表现最好的预测模型和准确率\n",
    "def max_score(df):\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    # 划分训练集和测试集\n",
    "    d = 10\n",
    "    X_train, X_test = X[:-d], X[-d:]\n",
    "    y_train = y[:-d]\n",
    "    y_test = y[-d:].tolist()\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    model_score = [] # 存储模型和准确率的元组\n",
    "    c=0.2 # 调整测试集中临近预测目标的日期的权重\n",
    "    model, y_pred = LDA(X_train, X_test, y_train)\n",
    "    model_score.append((model,adjust_score(y_pred,y_test, c)))\n",
    "    model, y_pred = DT(X_train, X_test, y_train)\n",
    "    model_score.append((model,adjust_score(y_pred,y_test, c)))\n",
    "    model, y_pred = GNB(X_train, X_test, y_train)\n",
    "    model_score.append((model,adjust_score(y_pred,y_test, c)))\n",
    "    model, y_pred = QDA(X_train, X_test, y_train)\n",
    "    model_score.append((model,adjust_score(y_pred,y_test, c)))\n",
    "    model, y_pred = RFC(X_train, X_test, y_train)\n",
    "    model_score.append((model,adjust_score(y_pred,y_test, c)))\n",
    "    model, y_pred = MLP(X_train, X_test, y_train)\n",
    "    model_score.append((model,adjust_score(y_pred,y_test, c)))\n",
    "    predictability = max([x[1] for x in model_score])\n",
    "    best_model = [x[0] for x in model_score if x[1] == predictability][0]\n",
    "    return best_model, scaler, predictability\n",
    "\n",
    "# 输入：待预测的日期，Top-n%，网络中各节点的densities字典；输出：Top-n%的股票在该日期价格变化的标签（二分类）\n",
    "def PLD(predict_date, n, densities):\n",
    "    stock_privilege_predict = []\n",
    "    for i in stocks:\n",
    "        df = pd.read_csv(path+i)\n",
    "        df = df[df['Date']<=predict_date].sort_values('Date').tail(100)\n",
    "        df = process_feature(df, 'PLD').tail(70) # 取值与T有关，即观察窗口， 目前选择的是观察最后10天结果\n",
    "        df['target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "        best_model, scaler, predictability = max_score(df[:-1])\n",
    "        privilege = densities[i.split('.')[0]] * predictability\n",
    "        X_predict = df.drop('target', axis=1).tail(1)\n",
    "        X_predict = scaler.transform(X_predict)\n",
    "        y_predict = best_model.predict(X_predict)[0]\n",
    "        stock_privilege_predict.append((i, privilege, y_predict))\n",
    "    # 按照 privilege 逆序排序\n",
    "    stock_privilege_predict_sorted = sorted(stock_privilege_predict, key=lambda x: x[1], reverse=True)\n",
    "    # 取前百分之 n 的数据\n",
    "    top_n = stock_privilege_predict_sorted[:int(len(stock_privilege_predict) * n / 100)]\n",
    "    prediction = {(stock.split('.')[0]): predict for stock, _, predict in top_n}\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将实验所需各月份的graph提前生成好并保存下来\n",
    "for i in ['2020-09-01','2020-10-01','2020-11-01','2020-12-01',]:#'2020-10-01',\n",
    "    G = create_graph(i) # 可以以月更新\n",
    "    normalize_graph(G)\n",
    "    # 将图保存到文件中\n",
    "    with open('graph_'+i+'.pkl', 'wb') as f:\n",
    "        pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# 输入：2002-10-30形式的两个字符串，代表首和尾，输出包含首尾的中间所有日期字符串\n",
    "def between_dates(start_date,end_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    end_date = datetime.datetime.strptime( end_date , '%Y-%m-%d').date()\n",
    "    # 计算日期间隔\n",
    "    delta = end_date - start_date\n",
    "    date_list = []\n",
    "    # 遍历日期间隔，生成所有日期，并以\"yyyy-MM-dd\"格式输出\n",
    "    for i in range(delta.days + 1):\n",
    "        date = start_date + datetime.timedelta(days=i)\n",
    "        date_list.append(date.strftime('%Y-%m-%d'))\n",
    "    return date_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DGL实现GCN\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义GCN模型\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        # 第一层GCN\n",
    "        self.conv1 = dgl.nn.GraphConv(11, 4, weight=True) # 11就是每个节点特征向量的长度\n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        # 第二层GCN\n",
    "        self.conv2 = dgl.nn.GraphConv(4, 4, weight=True)\n",
    "        # Dropout\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        # 第三层GCN\n",
    "        self.conv3 = dgl.nn.GraphConv(4, 2,weight=True)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot uniform initializer初始化\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.conv1.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.conv3.weight, gain=gain)\n",
    "\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        # 第一层GCN\n",
    "        h = self.conv1(g, x)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout1(h)\n",
    "        # 第二层GCN\n",
    "        h = self.conv2(g, h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout2(h)\n",
    "        # 第三层GCN\n",
    "        h = self.conv3(g, h)\n",
    "        return F.softmax(h, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gcn(features):\n",
    "    features = features.float()\n",
    "    # 计算节点特征的均值和方差\n",
    "    feat_mean = torch.mean(features, dim=0)\n",
    "    feat_std = torch.std(features, dim=0)\n",
    "    # 对节点特征进行标准化处理\n",
    "    features_normalized = (features - feat_mean) / (feat_std + 1e-8)\n",
    "    return features_normalized\n",
    "\n",
    "# 训练过程\n",
    "def train_gcn(gcn, n_epochs, g):\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "    # 定义损失函数和评估指标\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for epoch in range(n_epochs):\n",
    "        gcn.train()  # 进入训练模式\n",
    "        # features_normalized = normalize_gcn(g.ndata['feat'])\n",
    "        # logits = gcn(g, features_normalized)\n",
    "        logits = gcn(g, g.ndata['feat'])\n",
    "        mask = g.ndata['label'] >= 0\n",
    "        loss = loss_fn(logits[mask], g.ndata['label'][mask])\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = (torch.argmax(logits[mask], dim=1) == g.ndata['label'][mask]).float().mean().item()\n",
    "        # print(\"Epoch {:05d} | Loss {:.4f} | Acc {:.4f}\".format(epoch, loss.item(), acc))\n",
    "\n",
    "# 输入：torch形式的labels\n",
    "def test_gcn(gcn, g, labels):\n",
    "    gcn.eval()  # 进入评估模式\n",
    "    with torch.no_grad():\n",
    "        # 进行标准化处理\n",
    "        # features_normalized = normalize_gcn(g.ndata['feat'])\n",
    "        logits = gcn(g, g.ndata['feat'])  # 前向传播计算输出\n",
    "        # 输出所有节点的预测结果（包括未知标签的节点）\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # print(preds)\n",
    "        acc = (preds == labels).float().mean().item()\n",
    "        # print(f'Accuracy: {acc:.4f}')\n",
    "        # 计算MCC\n",
    "        from sklearn.metrics import matthews_corrcoef\n",
    "        mcc = matthews_corrcoef(labels.numpy(), preds.numpy())\n",
    "        # print(f'MCC: {mcc:.4f}')\n",
    "        return acc, mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tran_G(g_nx):\n",
    "    # 将节点名称映射到整数ID，并创建一个权重列表\n",
    "    id_dict = {n: i for i, n in enumerate(g_nx.nodes)}\n",
    "    weight_list = [e[2]['weight'] for e in g_nx.edges(data=True)]\n",
    "    edges = [(id_dict[e[0]], id_dict[e[1]]) for e in g_nx.edges]\n",
    "    # 创建一个DGLGraph对象并添加边和权重\n",
    "    g_dgl = dgl.DGLGraph(edges)\n",
    "    g_dgl.add_edges(*zip(*[(t, s) for s, t in edges]))\n",
    "    g_dgl.edata['weight'] = torch.tensor(weight_list+weight_list)\n",
    "    return g_dgl\n",
    "\n",
    "import pickle\n",
    "def experiment_gcn(predict_date, epoch_num, n):\n",
    "    # 从文件中加载图\n",
    "    with open('graph_'+predict_date[:7]+'-01.pkl', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "    id_dict = {n: i for i, n in enumerate(G.nodes)}\n",
    "    g_1 = tran_G(G) # t-4\n",
    "    # 定义一个字典，用于存储每个节点对应的特征向量\n",
    "    node_feats_1 = {}\n",
    "    node_labels_1 = {}\n",
    "    g_2 = tran_G(G) # t-3\n",
    "    node_feats_2 = {}\n",
    "    node_labels_2 = {}\n",
    "    g_3 = tran_G(G) # t-2\n",
    "    node_feats_3 = {}\n",
    "    node_labels_3 = {}\n",
    "    g_4 = tran_G(G) # t-1\n",
    "    node_feats_4 = {}\n",
    "    node_labels_4 = {}\n",
    "    g_5 = tran_G(G) # t\n",
    "    node_feats_5 = {}\n",
    "    node_labels_5 = {}\n",
    "    densities = density_score(G)\n",
    "    # n=20 # 20%=1/5\n",
    "    prediction = PLD(predict_date, n, densities)\n",
    "    for i in stocks:\n",
    "        df = pd.read_csv(path+i).sort_values('Date')\n",
    "        df = df[df['Date'] <= predict_date]\n",
    "        df = process_feature(df,'node_representation')\n",
    "        i = i.split('.')[0]\n",
    "        node_labels_1[id_dict[i]] = df[['label']].iloc[-5][0]\n",
    "        node_labels_2[id_dict[i]] = df[['label']].iloc[-4][0]\n",
    "        node_labels_3[id_dict[i]] = df[['label']].iloc[-3][0]\n",
    "        node_labels_4[id_dict[i]] = df[['label']].iloc[-2][0]\n",
    "        try:\n",
    "            node_labels_5[id_dict[i]] = prediction[i] # 属于PLD选择出的带标签的节点\n",
    "        except:\n",
    "            node_labels_5[id_dict[i]] = -1 # 不属于\n",
    "        node_feats_1[id_dict[i]] = torch.tensor(df[['RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S']].iloc[-5].values).view(11)\n",
    "        node_feats_2[id_dict[i]] = torch.tensor(df[['RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S']].iloc[-4].values).view(11)\n",
    "        node_feats_3[id_dict[i]] = torch.tensor(df[['RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S']].iloc[-3].values).view(11)\n",
    "        node_feats_4[id_dict[i]] = torch.tensor(df[['RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S']].iloc[-2].values).view(11)\n",
    "        node_feats_5[id_dict[i]] = torch.tensor(df[['RSI_S', 'BB_S', 'MACD_S', 'SAR_S', 'ADX_S', 'S_S', 'MFI_S', 'CCI_S', 'V_S', 'CPOP_S', 'CPCPY_S']].iloc[-1].values).view(11)\n",
    "    # 将每个节点的特征向量作为 'feat' 属性存入 Graph 对象中的每个节点\n",
    "    g_1.ndata['feat'] = torch.stack([node_feats_1[i] for i in range(len(stocks))])\n",
    "    g_2.ndata['feat'] = torch.stack([node_feats_2[i] for i in range(len(stocks))])\n",
    "    g_3.ndata['feat'] = torch.stack([node_feats_3[i] for i in range(len(stocks))])\n",
    "    g_4.ndata['feat'] = torch.stack([node_feats_4[i] for i in range(len(stocks))])\n",
    "    g_5.ndata['feat'] = torch.stack([node_feats_5[i] for i in range(len(stocks))])\n",
    "    # 将每个节点的 label 作为 'label' 属性存入 Graph 对象中的每个节点\n",
    "    g_1.ndata['label'] = torch.tensor([node_labels_1[i] for i in range(len(stocks))])\n",
    "    g_2.ndata['label'] = torch.tensor([node_labels_2[i] for i in range(len(stocks))])\n",
    "    g_3.ndata['label'] = torch.tensor([node_labels_3[i] for i in range(len(stocks))])\n",
    "    g_4.ndata['label'] = torch.tensor([node_labels_4[i] for i in range(len(stocks))])\n",
    "    g_5.ndata['label'] = torch.tensor([node_labels_5[i] for i in range(len(stocks))])\n",
    "\n",
    "    # 创建模型实例\n",
    "    gcn = GCN()\n",
    "    gcn.reset_parameters()\n",
    "    train_gcn(gcn, epoch_num, g_1)\n",
    "    train_gcn(gcn, epoch_num, g_2)\n",
    "    train_gcn(gcn, epoch_num, g_3)\n",
    "    train_gcn(gcn, epoch_num, g_4)\n",
    "    train_gcn(gcn, epoch_num, g_5)\n",
    "    labels = {}\n",
    "    for i in stocks:\n",
    "        df = pd.read_csv(path+i).sort_values('Date').tail(100)\n",
    "        df['label'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "        labels[id_dict[i.split('.')[0]]] = df[df['Date']==predict_date]['label'].values[0]\n",
    "    values = [labels[key] for key in sorted(labels.keys())]\n",
    "    labels = torch.tensor(values)\n",
    "    acc, mcc = test_gcn(gcn, g_5, labels)\n",
    "    return acc, mcc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+stocks[0])\n",
    "date_list = df[(df['Date']>='2020-12-01') & (df['Date']<'2020-12-31')].Date.tolist()\n",
    "\n",
    "experiment_gcn(predict_date, epoch_num, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+stocks[0])\n",
    "date_list = df[(df['Date']>='2020-09-30') & (df['Date']<'2020-12-31')].Date.tolist()\n",
    "\n",
    "accs = []  \n",
    "mccs = []\n",
    "for predict_date in date_list:\n",
    "    for i in range(10): # 重复10组独立实验\n",
    "        acc, mcc = experiment_gcn(predict_date, 30, 20)\n",
    "        accs.append(acc)\n",
    "        mccs.append(mcc)\n",
    "print((sum(accs) / len(accs), sum(mccs) / len(mccs))) \n",
    "# (56.07,0.0016)\n",
    "# 用时6小时"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
